#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡çŒ®ç­›é€‰ç¨‹åº
æ ¹æ®ç­›é€‰æ¡ä»¶è¿‡æ»¤PubMedæ£€ç´¢ç»“æœï¼ŒåŒ¹é…æœŸåˆŠæ•°æ®
"""

import pandas as pd
import json
import os
import pickle  # æ·»åŠ pickleç”¨äºç¼“å­˜
from typing import List, Dict, Optional, Tuple
from intent_analyzer import SearchCriteria
import re
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import hashlib
from datetime import datetime, timedelta


class FilterConfig:
    """ç­›é€‰å™¨é…ç½®ç±»"""
    def __init__(self):
        self.max_workers = 4  # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        self.cache_size = 1000  # ç¼“å­˜å¤§å°
        self.cache_ttl = 3600  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        self.batch_size = 200  # æ‰¹å¤„ç†å¤§å°
        self.enable_parallel = True  # å¯ç”¨å¹¶è¡Œå¤„ç†
        self.enable_caching = True  # å¯ç”¨ç¼“å­˜
        self.memory_limit_mb = 500  # å†…å­˜é™åˆ¶ï¼ˆMBï¼‰


class JournalInfoCache:
    """æœŸåˆŠä¿¡æ¯ç¼“å­˜ç®¡ç†å™¨"""
    def __init__(self, config: FilterConfig):
        self.config = config
        self.cache = {}
        self.access_times = {}
        self.lock = threading.Lock()
        self.stats = {'hits': 0, 'misses': 0, 'evictions': 0}
    
    def _generate_key(self, issn: str, eissn: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{issn}:{eissn}"
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
    
    def get(self, issn: str, eissn: str) -> Optional[Dict]:
        """è·å–æœŸåˆŠä¿¡æ¯"""
        if not self.config.enable_caching:
            return None
            
        key = self._generate_key(issn, eissn)
        
        with self.lock:
            if key in self.cache:
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if time.time() - self.access_times[key] < self.config.cache_ttl:
                    self.access_times[key] = time.time()
                    self.stats['hits'] += 1
                    return self.cache[key]
                else:
                    # è¿‡æœŸåˆ é™¤
                    del self.cache[key]
                    del self.access_times[key]
                    self.stats['evictions'] += 1
            
            self.stats['misses'] += 1
            return None
    
    def put(self, issn: str, eissn: str, info: Dict):
        """å­˜å‚¨æœŸåˆŠä¿¡æ¯"""
        if not self.config.enable_caching:
            return
            
        key = self._generate_key(issn, eissn)
        
        with self.lock:
            # æ£€æŸ¥ç¼“å­˜å¤§å°
            if len(self.cache) >= self.config.cache_size:
                # LRUæ·˜æ±°
                oldest_key = min(self.access_times.keys(), key=self.access_times.get)
                del self.cache[oldest_key]
                del self.access_times[oldest_key]
                self.stats['evictions'] += 1
            
            self.cache[key] = info
            self.access_times[key] = time.time()
    
    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.stats['hits'] + self.stats['misses']
        hit_rate = self.stats['hits'] / total_requests if total_requests > 0 else 0
        
        return {
            'cache_size': len(self.cache),
            'max_cache_size': self.config.cache_size,
            'hits': self.stats['hits'],
            'misses': self.stats['misses'],
            'evictions': self.stats['evictions'],
            'hit_rate': hit_rate
        }


class LiteratureFilter:
    """æ–‡çŒ®ç­›é€‰å™¨"""
    
    def __init__(self, zky_data_path: str = "data/processed_zky_data.csv", 
                 jcr_data_path: str = "data/processed_jcr_data.csv",
                 config: FilterConfig = None):
        """
        åˆå§‹åŒ–æ–‡çŒ®ç­›é€‰å™¨
        
        Args:
            zky_data_path: ä¸­ç§‘é™¢æ•°æ®æ–‡ä»¶è·¯å¾„
            jcr_data_path: JCRæ•°æ®æ–‡ä»¶è·¯å¾„
            config: ç­›é€‰å™¨é…ç½®
        """
        self.zky_data_path = zky_data_path
        self.jcr_data_path = jcr_data_path
        self.config = config or FilterConfig()
        
        # åˆå§‹åŒ–ç¼“å­˜
        self.journal_cache = JournalInfoCache(self.config)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_articles_processed': 0,
            'total_filter_time': 0,
            'cache_hits': 0,
            'parallel_batches': 0,
            'memory_usage_mb': 0,
            'errors': 0
        }
        
        # åŠ è½½æœŸåˆŠæ•°æ®
        self.zky_data = self._load_zky_data_optimized()
        self.jcr_data = self._load_jcr_data_optimized()
        
        # åˆ›å»ºISSNåˆ°æœŸåˆŠä¿¡æ¯çš„æ˜ å°„ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
        self.issn_to_journal_info = self._load_or_build_journal_mapping()
        
        print(f"å·²åŠ è½½ä¸­ç§‘é™¢æ•°æ®: {len(self.zky_data)} æ¡è®°å½•")
        print(f"å·²åŠ è½½JCRæ•°æ®: {len(self.jcr_data)} æ¡è®°å½•")
        print(f"æœŸåˆŠæ˜ å°„è¡¨: {len(self.issn_to_journal_info)} æ¡è®°å½•")
        print(f"å¹¶è¡Œå¤„ç†: {'å¯ç”¨' if self.config.enable_parallel else 'ç¦ç”¨'}")
        print(f"ç¼“å­˜ç³»ç»Ÿ: {'å¯ç”¨' if self.config.enable_caching else 'ç¦ç”¨'}")
    
    def _load_zky_data_optimized(self) -> pd.DataFrame:
        """ä¼˜åŒ–çš„ä¸­ç§‘é™¢æ•°æ®åŠ è½½æ–¹æ³•"""
        try:
            if not os.path.exists(self.zky_data_path):
                print(f"ä¸­ç§‘é™¢æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.zky_data_path}")
                return pd.DataFrame()
            
            # ä½¿ç”¨åˆ†å—è¯»å–å‡å°‘å†…å­˜ä½¿ç”¨
            chunks = []
            for chunk in pd.read_csv(self.zky_data_path, encoding='utf-8', chunksize=1000):
                # æ•°æ®æ¸…æ´—å’Œä¼˜åŒ–
                chunk = self._clean_journal_data(chunk)
                chunks.append(chunk)
                
                # å†…å­˜æ£€æŸ¥
                if self._check_memory_limit():
                    print("å†…å­˜ä½¿ç”¨æ¥è¿‘é™åˆ¶ï¼Œåœæ­¢åŠ è½½æ•°æ®")
                    break
            
            if chunks:
                df = pd.concat(chunks, ignore_index=True)
                print(f"æˆåŠŸåŠ è½½ä¸­ç§‘é™¢æ•°æ®: {len(df)} æ¡è®°å½•")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            print(f"åŠ è½½ä¸­ç§‘é™¢æ•°æ®å¤±è´¥: {e}")
            self.performance_stats['errors'] += 1
            return pd.DataFrame()
    
    def _load_zky_data(self) -> pd.DataFrame:
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self._load_zky_data_optimized()
    
    def _load_jcr_data_optimized(self) -> pd.DataFrame:
        """ä¼˜åŒ–çš„JCRæ•°æ®åŠ è½½æ–¹æ³•"""
        try:
            if not os.path.exists(self.jcr_data_path):
                print(f"JCRæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.jcr_data_path}")
                return pd.DataFrame()
            
            # ä½¿ç”¨åˆ†å—è¯»å–å‡å°‘å†…å­˜ä½¿ç”¨
            chunks = []
            for chunk in pd.read_csv(self.jcr_data_path, encoding='utf-8', chunksize=1000):
                # æ•°æ®æ¸…æ´—å’Œä¼˜åŒ–
                chunk = self._clean_journal_data(chunk)
                chunks.append(chunk)
                
                # å†…å­˜æ£€æŸ¥
                if self._check_memory_limit():
                    print("å†…å­˜ä½¿ç”¨æ¥è¿‘é™åˆ¶ï¼Œåœæ­¢åŠ è½½æ•°æ®")
                    break
            
            if chunks:
                df = pd.concat(chunks, ignore_index=True)
                print(f"æˆåŠŸåŠ è½½JCRæ•°æ®: {len(df)} æ¡è®°å½•")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            print(f"åŠ è½½JCRæ•°æ®å¤±è´¥: {e}")
            self.performance_stats['errors'] += 1
            return pd.DataFrame()
    
    def _load_jcr_data(self) -> pd.DataFrame:
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self._load_jcr_data_optimized()
    
    def _clean_journal_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æœŸåˆŠæ•°æ®"""
        try:
            # æ ‡å‡†åŒ–ISSNæ ¼å¼
            if 'ISSN' in df.columns:
                df['ISSN'] = df['ISSN'].astype(str).str.strip()
                df['ISSN'] = df['ISSN'].replace('nan', '')
            
            if 'EISSN' in df.columns:
                df['EISSN'] = df['EISSN'].astype(str).str.strip()
                df['EISSN'] = df['EISSN'].replace('nan', '')
            
            # å¤„ç†æ•°å€¼å­—æ®µ
            numeric_columns = ['å½±å“å› å­', 'ä¸­ç§‘é™¢åˆ†åŒº']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            return df
        except Exception as e:
            print(f"æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            return df
    
    def _check_memory_limit(self) -> bool:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æ˜¯å¦æ¥è¿‘é™åˆ¶"""
        try:
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.performance_stats['memory_usage_mb'] = memory_mb
            return memory_mb > self.config.memory_limit_mb * 0.8
        except ImportError:
            return False
        except Exception:
            return False
    
    def _build_journal_mapping_optimized(self) -> Dict[str, Dict]:
        """ä¼˜åŒ–çš„ISSNåˆ°æœŸåˆŠä¿¡æ¯æ˜ å°„æ„å»ºæ–¹æ³•"""
        start_time = time.time()
        mapping = {}
        
        print("[INFO] å¼€å§‹æ„å»ºæœŸåˆŠæ˜ å°„è¡¨...")
        
        # å¹¶è¡Œå¤„ç†ä¸­ç§‘é™¢æ•°æ®
        if not self.zky_data.empty:
            print(f"[INFO] å¤„ç†ä¸­ç§‘é™¢æ•°æ® ({len(self.zky_data)} æ¡è®°å½•)...")
            zky_mapping = self._process_dataframe_parallel(self.zky_data, 'zky')
            mapping.update(zky_mapping)
            print(f"[INFO] ä¸­ç§‘é™¢æ•°æ®å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(zky_mapping)} ä¸ªæ˜ å°„æ¡ç›®")
        
        # å¹¶è¡Œå¤„ç†JCRæ•°æ®
        if not self.jcr_data.empty:
            print(f"[INFO] å¤„ç†JCRæ•°æ® ({len(self.jcr_data)} æ¡è®°å½•)...")
            jcr_mapping = self._process_dataframe_parallel(self.jcr_data, 'jcr')
            # åˆå¹¶ä¿¡æ¯ï¼Œä¸è¦†ç›–å·²æœ‰æ•°æ®
            print(f"[INFO] åˆå¹¶JCRæ•°æ®åˆ°æ˜ å°„è¡¨...")
            for issn, info in jcr_mapping.items():
                if issn in mapping:
                    mapping[issn].update({k: v for k, v in info.items() if v is not None})
                else:
                    mapping[issn] = info
            print(f"[INFO] JCRæ•°æ®å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(jcr_mapping)} ä¸ªæ˜ å°„æ¡ç›®")
        
        build_time = time.time() - start_time
        print(f"[OK] æœŸåˆŠæ˜ å°„æ„å»ºå®Œæˆï¼Œæ€»è®¡ {len(mapping)} ä¸ªæ˜ å°„æ¡ç›®ï¼Œè€—æ—¶: {build_time:.2f}ç§’")
        
        return mapping
    
    def _process_dataframe_parallel(self, df: pd.DataFrame, data_type: str) -> Dict[str, Dict]:
        """å¹¶è¡Œå¤„ç†DataFrameæ„å»ºæ˜ å°„"""
        mapping = {}
        
        if self.config.enable_parallel and len(df) > 1000:
            # åˆ†å—å¹¶è¡Œå¤„ç†
            chunk_size = min(1000, len(df) // self.config.max_workers)
            chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]
            
            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                futures = []
                for chunk in chunks:
                    future = executor.submit(self._process_chunk, chunk, data_type)
                    futures.append(future)
                
                for future in as_completed(futures):
                    try:
                        chunk_mapping = future.result()
                        mapping.update(chunk_mapping)
                    except Exception as e:
                        print(f"å¤„ç†æ•°æ®å—å¤±è´¥: {e}")
                        self.performance_stats['errors'] += 1
        else:
            # ä¸²è¡Œå¤„ç†
            mapping = self._process_chunk(df, data_type)
        
        return mapping
    
    def _process_chunk(self, df: pd.DataFrame, data_type: str) -> Dict[str, Dict]:
        """å¤„ç†æ•°æ®å— - å®Œå…¨å‘é‡åŒ–ç‰ˆæœ¬"""
        mapping = {}
        
        if data_type == 'zky':
            # å¤„ç†ä¸­ç§‘é™¢æ•°æ®
            if 'ISSN' in df.columns:
                issns = df['ISSN'].astype(str).str.strip()
                cas_zones = df['ä¸­ç§‘é™¢åˆ†åŒº'] if 'ä¸­ç§‘é™¢åˆ†åŒº' in df.columns else None
                
                # è¿‡æ»¤æœ‰æ•ˆçš„ISSN
                valid_mask = (issns != '') & (issns != 'nan') & issns.notna()
                valid_issns = issns[valid_mask]
                
                if cas_zones is not None:
                    valid_cas_zones = cas_zones[valid_mask]
                    for issn, cas_zone in zip(valid_issns, valid_cas_zones):
                        mapping[issn] = {
                            'cas_zone': cas_zone if pd.notna(cas_zone) else None,
                            'impact_factor': None,
                            'jcr_quartile': None
                        }
                else:
                    for issn in valid_issns:
                        mapping[issn] = {
                            'cas_zone': None,
                            'impact_factor': None,
                            'jcr_quartile': None
                        }
            
            # å¤„ç†EISSN
            if 'EISSN' in df.columns:
                eissns = df['EISSN'].astype(str).str.strip()
                cas_zones = df['ä¸­ç§‘é™¢åˆ†åŒº'] if 'ä¸­ç§‘é™¢åˆ†åŒº' in df.columns else None
                
                valid_mask = (eissns != '') & (eissns != 'nan') & eissns.notna()
                valid_eissns = eissns[valid_mask]
                
                if cas_zones is not None:
                    valid_cas_zones = cas_zones[valid_mask]
                    for eissn, cas_zone in zip(valid_eissns, valid_cas_zones):
                        mapping[eissn] = {
                            'cas_zone': cas_zone if pd.notna(cas_zone) else None,
                            'impact_factor': None,
                            'jcr_quartile': None
                        }
                else:
                    for eissn in valid_eissns:
                        mapping[eissn] = {
                            'cas_zone': None,
                            'impact_factor': None,
                            'jcr_quartile': None
                        }
                        
        else:  # jcræ•°æ®
            # å¤„ç†JCRæ•°æ®
            if 'ISSN' in df.columns:
                issns = df['ISSN'].astype(str).str.strip()
                impact_factors = df['å½±å“å› å­'] if 'å½±å“å› å­' in df.columns else None
                jcr_quartiles = df['JCRåˆ†åŒº'] if 'JCRåˆ†åŒº' in df.columns else None
                
                valid_mask = (issns != '') & (issns != 'nan') & issns.notna()
                valid_issns = issns[valid_mask]
                
                if impact_factors is not None and jcr_quartiles is not None:
                    valid_ifs = impact_factors[valid_mask]
                    valid_quartiles = jcr_quartiles[valid_mask]
                    for issn, impact_factor, jcr_quartile in zip(valid_issns, valid_ifs, valid_quartiles):
                        mapping[issn] = {
                            'cas_zone': None,
                            'impact_factor': impact_factor if pd.notna(impact_factor) else None,
                            'jcr_quartile': jcr_quartile if pd.notna(jcr_quartile) else None
                        }
                else:
                    for issn in valid_issns:
                        mapping[issn] = {
                            'cas_zone': None,
                            'impact_factor': None,
                            'jcr_quartile': None
                        }
            
            # å¤„ç†EISSN
            if 'EISSN' in df.columns:
                eissns = df['EISSN'].astype(str).str.strip()
                impact_factors = df['å½±å“å› å­'] if 'å½±å“å› å­' in df.columns else None
                jcr_quartiles = df['JCRåˆ†åŒº'] if 'JCRåˆ†åŒº' in df.columns else None
                
                valid_mask = (eissns != '') & (eissns != 'nan') & eissns.notna()
                valid_eissns = eissns[valid_mask]
                
                if impact_factors is not None and jcr_quartiles is not None:
                    valid_ifs = impact_factors[valid_mask]
                    valid_quartiles = jcr_quartiles[valid_mask]
                    for eissn, impact_factor, jcr_quartile in zip(valid_eissns, valid_ifs, valid_quartiles):
                        mapping[eissn] = {
                            'cas_zone': None,
                            'impact_factor': impact_factor if pd.notna(impact_factor) else None,
                            'jcr_quartile': jcr_quartile if pd.notna(jcr_quartile) else None
                        }
                else:
                    for eissn in valid_eissns:
                        mapping[eissn] = {
                            'cas_zone': None,
                            'impact_factor': None,
                            'jcr_quartile': None
                        }
        
        return mapping
    
    def _build_journal_mapping(self) -> Dict[str, Dict]:
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self._build_journal_mapping_optimized()
    
    def _get_mapping_cache_path(self) -> str:
        """è·å–æœŸåˆŠæ˜ å°„è¡¨ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return "cache/journal_mapping_cache.pkl"
    
    def _get_data_files_hash(self) -> str:
        """è®¡ç®—æ•°æ®æ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹æ–‡ä»¶æ˜¯å¦æœ‰æ›´æ–°"""
        hash_obj = hashlib.md5()
        
        # è®¡ç®—ä¸­ç§‘é™¢æ•°æ®æ–‡ä»¶çš„å“ˆå¸Œ
        if os.path.exists(self.zky_data_path):
            with open(self.zky_data_path, 'rb') as f:
                hash_obj.update(f.read())
        
        # è®¡ç®—JCRæ•°æ®æ–‡ä»¶çš„å“ˆå¸Œ
        if os.path.exists(self.jcr_data_path):
            with open(self.jcr_data_path, 'rb') as f:
                hash_obj.update(f.read())
        
        return hash_obj.hexdigest()
    
    def _load_mapping_cache(self) -> Optional[Dict[str, Dict]]:
        """åŠ è½½æœŸåˆŠæ˜ å°„è¡¨ç¼“å­˜"""
        cache_path = self._get_mapping_cache_path()
        
        if not os.path.exists(cache_path):
            return None
        
        try:
            with open(cache_path, 'rb') as f:
                cache_data = pickle.load(f)
            
            # æ£€æŸ¥ç¼“å­˜çš„æ•°æ®ç‰ˆæœ¬
            current_hash = self._get_data_files_hash()
            cached_hash = cache_data.get('data_hash', '')
            cached_time = cache_data.get('cached_at', 0)
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ90å¤©ï¼Œ3ä¸ªæœˆï¼‰
            cache_age_days = (time.time() - cached_time) / (24 * 3600)
            
            if current_hash != cached_hash:
                print(f"[CACHE] æ•°æ®æ–‡ä»¶å·²æ›´æ–°ï¼Œç¼“å­˜å¤±æ•ˆ")
                return None
            elif cache_age_days > 90:
                print(f"[CACHE] ç¼“å­˜å·²è¿‡æœŸ ({cache_age_days:.1f}å¤©)ï¼Œå°†é‡æ–°æ„å»º")
                return None
            else:
                print(f"[CACHE] ä½¿ç”¨æœŸåˆŠæ˜ å°„è¡¨ç¼“å­˜ (ç¼“å­˜æ—¶é—´: {cache_age_days:.1f}å¤©)")
                return cache_data['mapping']
                
        except Exception as e:
            print(f"[CACHE] åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def _save_mapping_cache(self, mapping: Dict[str, Dict]):
        """ä¿å­˜æœŸåˆŠæ˜ å°„è¡¨ç¼“å­˜"""
        cache_path = self._get_mapping_cache_path()
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        
        try:
            cache_data = {
                'mapping': mapping,
                'data_hash': self._get_data_files_hash(),
                'cached_at': time.time(),
                'version': '1.0'
            }
            
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)
                
            print(f"[CACHE] æœŸåˆŠæ˜ å°„è¡¨ç¼“å­˜å·²ä¿å­˜")
            
        except Exception as e:
            print(f"[CACHE] ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _load_or_build_journal_mapping(self) -> Dict[str, Dict]:
        """åŠ è½½æˆ–æ„å»ºæœŸåˆŠæ˜ å°„è¡¨ï¼ˆæ”¯æŒç¼“å­˜ï¼‰"""
        # å°è¯•åŠ è½½ç¼“å­˜
        cached_mapping = self._load_mapping_cache()
        if cached_mapping is not None:
            return cached_mapping
        
        # ç¼“å­˜ä¸å­˜åœ¨æˆ–å¤±æ•ˆï¼Œé‡æ–°æ„å»º
        print("[INFO] æ„å»ºæ–°çš„æœŸåˆŠæ˜ å°„è¡¨...")
        mapping = self._build_journal_mapping_optimized()
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_mapping_cache(mapping)
        
        return mapping
    
    def clear_mapping_cache(self):
        """æ¸…ç†æœŸåˆŠæ˜ å°„è¡¨ç¼“å­˜"""
        cache_path = self._get_mapping_cache_path()
        if os.path.exists(cache_path):
            try:
                os.remove(cache_path)
                print("[CACHE] æœŸåˆŠæ˜ å°„è¡¨ç¼“å­˜å·²æ¸…ç†")
            except Exception as e:
                print(f"[CACHE] æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
        else:
            print("[CACHE] ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨")
    
    def get_journal_info_optimized(self, issn: str, eissn: str) -> Dict:
        """
        ä¼˜åŒ–çš„æœŸåˆŠä¿¡æ¯è·å–æ–¹æ³•ï¼Œæ”¯æŒç¼“å­˜
        
        Args:
            issn: æœŸåˆŠISSN
            eissn: æœŸåˆŠeISSN
            
        Returns:
            æœŸåˆŠä¿¡æ¯å­—å…¸
        """
        # æ£€æŸ¥ç¼“å­˜
        cached_info = self.journal_cache.get(issn, eissn)
        if cached_info:
            self.performance_stats['cache_hits'] += 1
            return cached_info
        
        journal_info = {
            'cas_zone': None,
            'impact_factor': None,
            'jcr_quartile': None
        }
        
        # ä¼˜å…ˆä½¿ç”¨ISSNæŸ¥æ‰¾
        if issn and issn.strip():
            clean_issn = issn.strip()
            if clean_issn in self.issn_to_journal_info:
                info = self.issn_to_journal_info[clean_issn]
                journal_info.update({k: v for k, v in info.items() if v is not None})
        
        # å¦‚æœISSNæ²¡æ‰¾åˆ°æˆ–ä¿¡æ¯ä¸å®Œæ•´ï¼Œå°è¯•eISSN
        if eissn and eissn.strip():
            clean_eissn = eissn.strip()
            if clean_eissn in self.issn_to_journal_info:
                info = self.issn_to_journal_info[clean_eissn]
                # åªæ›´æ–°ä¸ºNoneçš„å­—æ®µ
                for key, value in info.items():
                    if journal_info[key] is None and value is not None:
                        journal_info[key] = value
        
        # ç¼“å­˜ç»“æœ
        self.journal_cache.put(issn, eissn, journal_info)
        
        return journal_info
    
    def get_journal_info(self, issn: str, eissn: str) -> Dict:
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self.get_journal_info_optimized(issn, eissn)
    
    def filter_articles_optimized(self, articles: List[Dict], criteria: SearchCriteria) -> List[Dict]:
        """
        ä¼˜åŒ–çš„æ–‡çŒ®ç­›é€‰æ–¹æ³•ï¼Œæ”¯æŒå¹¶è¡Œå¤„ç†
        
        Args:
            articles: æ–‡çŒ®åˆ—è¡¨
            criteria: ç­›é€‰æ¡ä»¶
            
        Returns:
            è¿‡æ»¤åçš„æ–‡çŒ®åˆ—è¡¨
        """
        start_time = time.time()
        
        print(f"\nå¼€å§‹ç­›é€‰æ–‡çŒ®ï¼ŒåŸå§‹æ–‡çŒ®æ•°: {len(articles)}")
        print(f"å¹¶è¡Œå¤„ç†: {'å¯ç”¨' if self.config.enable_parallel else 'ç¦ç”¨'}")
        
        # æ˜¾ç¤ºç­›é€‰æ¡ä»¶ï¼ˆæ’é™¤å·²åœ¨PubMedæ£€ç´¢ä¸­åº”ç”¨çš„æ¡ä»¶ï¼‰
        pubmed_filters = (criteria.year_start or criteria.year_end or criteria.keywords)
        journal_filters = (criteria.min_if or criteria.max_if or 
                          criteria.cas_zones or criteria.jcr_quartiles)
        
        if pubmed_filters:
            print(f"[PubMed] å·²åœ¨æ£€ç´¢ä¸­åº”ç”¨çš„æ¡ä»¶:")
            if criteria.year_start or criteria.year_end:
                print(f"  - å¹´ä»½: {criteria.year_start or 'ä¸é™'}-{criteria.year_end or 'ä¸é™'}")
            if criteria.keywords:
                print(f"  - å…³é”®è¯: {criteria.keywords}")
        
        if journal_filters:
            print(f"[FILTER] æœŸåˆŠç­›é€‰æ¡ä»¶:")
            if criteria.min_if or criteria.max_if:
                print(f"  - å½±å“å› å­: {criteria.min_if or 'ä¸é™'}-{criteria.max_if or 'ä¸é™'}")
            if criteria.cas_zones:
                print(f"  - ä¸­ç§‘é™¢åˆ†åŒº: {criteria.cas_zones}")
            if criteria.jcr_quartiles:
                print(f"  - JCRåˆ†åŒº: {criteria.jcr_quartiles}")
        
        if not pubmed_filters and not journal_filters:
            print("ç­›é€‰æ¡ä»¶: æ— ")
        
        filtered_articles = []
        
        if self.config.enable_parallel and len(articles) > self.config.batch_size:
            # å¹¶è¡Œå¤„ç†
            filtered_articles = self._filter_articles_parallel(articles, criteria)
        else:
            # ä¸²è¡Œå¤„ç†
            filtered_articles = self._filter_articles_serial(articles, criteria)
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        filter_time = time.time() - start_time
        self.performance_stats['total_filter_time'] += filter_time
        self.performance_stats['total_articles_processed'] += len(articles)
        
        print(f"ç­›é€‰åæ–‡çŒ®æ•°: {len(filtered_articles)}")
        print(f"ç­›é€‰è€—æ—¶: {filter_time:.2f}ç§’")
        
        # æ˜¾ç¤ºé”™è¯¯ç»Ÿè®¡
        if self.performance_stats['errors'] > 0:
            print(f"[WARN] å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿ {self.performance_stats['errors']} ä¸ªé”™è¯¯")
            print(f"[INFO] å·²ä¿ç•™æ‰€æœ‰æ–‡çŒ®ï¼ŒåŒ…æ‹¬å¤„ç†å¤±è´¥çš„æ–‡çŒ®")
        
        # ç»Ÿè®¡å¤„ç†çŠ¶æ€
        normal_articles = sum(1 for a in filtered_articles if '_processing_error' not in a)
        error_articles = sum(1 for a in filtered_articles if '_processing_error' in a)
        journal_error_articles = sum(1 for a in filtered_articles if '_journal_info_error' in a)
        
        if error_articles > 0:
            print(f"[STAT] å¤„ç†çŠ¶æ€: æ­£å¸¸ {normal_articles} ç¯‡, é”™è¯¯ {error_articles} ç¯‡")
        if journal_error_articles > 0:
            print(f"[STAT] æœŸåˆŠä¿¡æ¯è·å–å¤±è´¥: {journal_error_articles} ç¯‡")
        
        return filtered_articles
    
    def _filter_articles_parallel(self, articles: List[Dict], criteria: SearchCriteria) -> List[Dict]:
        """å¹¶è¡Œç­›é€‰æ–‡çŒ®"""
        filtered_articles = []
        
        # åˆ†æ‰¹å¤„ç†
        batch_size = self.config.batch_size
        batches = [articles[i:i + batch_size] for i in range(0, len(articles), batch_size)]
        
        print(f"åˆ† {len(batches)} æ‰¹å¹¶è¡Œå¤„ç†ï¼Œæ¯æ‰¹ {batch_size} ç¯‡")
        
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {
                executor.submit(self._process_batch, batch, criteria): batch 
                for batch in batches
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_batch):
                try:
                    batch_results = future.result()
                    filtered_articles.extend(batch_results)
                    self.performance_stats['parallel_batches'] += 1
                except Exception as e:
                    print(f"å¤„ç†æ‰¹æ¬¡å¤±è´¥: {e}")
                    self.performance_stats['errors'] += 1
        
        return filtered_articles
    
    def _filter_articles_serial(self, articles: List[Dict], criteria: SearchCriteria) -> List[Dict]:
        """ä¸²è¡Œç­›é€‰æ–‡çŒ®"""
        filtered_articles = []
        
        for article in articles:
            try:
                if self._meets_criteria(article, criteria):
                    enhanced_article = self._enhance_article_info(article)
                    filtered_articles.append(enhanced_article)
            except Exception as e:
                # é”™è¯¯å¤„ç†ï¼šå³ä½¿å¢å¼ºä¿¡æ¯å¤±è´¥ï¼Œä¹Ÿä¿ç•™åŸå§‹æ–‡çŒ®
                pmid = article.get('pmid', 'æœªçŸ¥')
                title = article.get('title', 'æœªçŸ¥æ ‡é¢˜')[:50]
                
                print(f"[WARN] æ–‡çŒ®å¤„ç†å¤±è´¥ (PMID: {pmid}): {e}")
                print(f"[INFO] ä¿ç•™åŸå§‹æ–‡çŒ®: {title}...")
                
                # è®°å½•é”™è¯¯ä½†ä¿ç•™æ–‡çŒ®
                self.performance_stats['errors'] += 1
                
                # å³ä½¿å¢å¼ºå¤±è´¥ï¼Œä¹Ÿè¦ä¿ç•™æ»¡è¶³æ¡ä»¶çš„æ–‡çŒ®
                try:
                    # ç®€åŒ–çš„æ¡ä»¶æ£€æŸ¥ï¼ˆä¸ä¾èµ–æœŸåˆŠä¿¡æ¯ï¼‰
                    if self._meets_criteria_basic(article, criteria):
                        # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡çŒ®ï¼Œä¸æ·»åŠ æœŸåˆŠä¿¡æ¯
                        filtered_articles.append(article)
                except Exception as e2:
                    print(f"[ERROR] æ–‡çŒ®æ¡ä»¶æ£€æŸ¥ä¹Ÿå¤±è´¥ (PMID: {pmid}): {e2}")
                    # æœ€ç»ˆä¿éšœï¼šæ— è®ºå¦‚ä½•éƒ½ä¿ç•™æ–‡çŒ®
                    article_copy = article.copy()
                    article_copy['_processing_error'] = str(e)
                    filtered_articles.append(article_copy)
        
        return filtered_articles
    
    def _process_batch(self, batch: List[Dict], criteria: SearchCriteria) -> List[Dict]:
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ–‡çŒ®"""
        batch_results = []
        
        for article in batch:
            try:
                if self._meets_criteria(article, criteria):
                    enhanced_article = self._enhance_article_info(article)
                    batch_results.append(enhanced_article)
            except Exception as e:
                # é”™è¯¯å¤„ç†ï¼šå³ä½¿å¢å¼ºä¿¡æ¯å¤±è´¥ï¼Œä¹Ÿä¿ç•™åŸå§‹æ–‡çŒ®
                pmid = article.get('pmid', 'æœªçŸ¥')
                title = article.get('title', 'æœªçŸ¥æ ‡é¢˜')[:50]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                
                print(f"[WARN] æ–‡çŒ®å¤„ç†å¤±è´¥ (PMID: {pmid}): {e}")
                print(f"[INFO] ä¿ç•™åŸå§‹æ–‡çŒ®: {title}...")
                
                # è®°å½•é”™è¯¯ä½†ä¿ç•™æ–‡çŒ®
                self.performance_stats['errors'] += 1
                
                # å³ä½¿å¢å¼ºå¤±è´¥ï¼Œä¹Ÿè¦ä¿ç•™æ»¡è¶³æ¡ä»¶çš„æ–‡çŒ®
                try:
                    # ç®€åŒ–çš„æ¡ä»¶æ£€æŸ¥ï¼ˆä¸ä¾èµ–æœŸåˆŠä¿¡æ¯ï¼‰
                    if self._meets_criteria_basic(article, criteria):
                        # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡çŒ®ï¼Œä¸æ·»åŠ æœŸåˆŠä¿¡æ¯
                        batch_results.append(article)
                except Exception as e2:
                    print(f"[ERROR] æ–‡çŒ®æ¡ä»¶æ£€æŸ¥ä¹Ÿå¤±è´¥ (PMID: {pmid}): {e2}")
                    # æœ€ç»ˆä¿éšœï¼šæ— è®ºå¦‚ä½•éƒ½ä¿ç•™æ–‡çŒ®
                    article_copy = article.copy()
                    article_copy['_processing_error'] = str(e)
                    batch_results.append(article_copy)
        
        return batch_results
    
    def filter_articles(self, articles: List[Dict], criteria: SearchCriteria) -> List[Dict]:
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self.filter_articles_optimized(articles, criteria)
    
    def _meets_criteria_basic(self, article: Dict, criteria: SearchCriteria) -> bool:
        """åŸºæœ¬æ¡ä»¶æ£€æŸ¥ï¼ˆä¸ä¾èµ–æœŸåˆŠä¿¡æ¯ï¼‰"""
        
          
          
        return True
    
    def _meets_criteria(self, article: Dict, criteria: SearchCriteria) -> bool:
        """æ£€æŸ¥æ–‡çŒ®æ˜¯å¦æ»¡è¶³ç­›é€‰æ¡ä»¶"""
        
        # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœæ²¡æœ‰è®¾ç½®ä»»ä½•ç­›é€‰æ¡ä»¶ï¼Œç›´æ¥è¿”å›True
        if (not criteria.min_if and not criteria.max_if and 
            not criteria.cas_zones and not criteria.jcr_quartiles):
            return True
        
        # è·å–æœŸåˆŠä¿¡æ¯
        issn = article.get('issn', '')
        eissn = article.get('eissn', '')
        journal_info = self.get_journal_info_optimized(issn, eissn)
        
          
        # æ£€æŸ¥å½±å“å› å­é™åˆ¶
        impact_factor = journal_info.get('impact_factor')
        if impact_factor:
            if criteria.min_if and impact_factor < criteria.min_if:
                return False
            if criteria.max_if and impact_factor > criteria.max_if:
                return False
        else:
            # å¦‚æœè¦æ±‚æœ‰å½±å“å› å­ä½†æ²¡æœ‰æ•°æ®ï¼Œåˆ™æ’é™¤
            if criteria.min_if or criteria.max_if:
                return False
        
        # æ£€æŸ¥ä¸­ç§‘é™¢åˆ†åŒºé™åˆ¶
        if criteria.cas_zones:
            cas_zone = journal_info.get('cas_zone')
            if not cas_zone:
                return False
            try:
                cas_zone_int = int(cas_zone)
                if cas_zone_int not in criteria.cas_zones:
                    return False
            except (ValueError, TypeError):
                return False
        
        # æ£€æŸ¥JCRåˆ†åŒºé™åˆ¶
        if criteria.jcr_quartiles:
            jcr_quartile = journal_info.get('jcr_quartile')
            if not jcr_quartile or str(jcr_quartile) not in criteria.jcr_quartiles:
                return False
        
          
        return True
    
    def _extract_year(self, pub_date: str) -> Optional[int]:
        """ä»å‘è¡¨æ—¥æœŸä¸­æå–å¹´ä»½"""
        if not pub_date:
            return None
        
        # å°è¯•æå–4ä½æ•°å­—å¹´ä»½
        year_match = re.search(r'(\d{4})', str(pub_date))
        if year_match:
            year = int(year_match.group(1))
            # åˆç†çš„å¹´ä»½èŒƒå›´
            if 1900 <= year <= 2030:
                return year
        
        return None
    
    def _enhance_article_info(self, article: Dict) -> Dict:
        """å¢å¼ºæ–‡çŒ®ä¿¡æ¯ï¼Œæ·»åŠ æœŸåˆŠæ•°æ®"""
        enhanced = article.copy()
        
        try:
            # è·å–æœŸåˆŠä¿¡æ¯
            issn = article.get('issn', '')
            eissn = article.get('eissn', '')
            journal_info = self.get_journal_info_optimized(issn, eissn)
            
            # å®‰å…¨åœ°æ·»åŠ æœŸåˆŠä¿¡æ¯
            enhanced['cas_zone'] = journal_info.get('cas_zone') if journal_info else None
            enhanced['impact_factor'] = journal_info.get('impact_factor') if journal_info else None
            enhanced['jcr_quartile'] = journal_info.get('jcr_quartile') if journal_info else None
            
        except Exception as e:
            # æœŸåˆŠä¿¡æ¯å¢å¼ºå¤±è´¥ï¼Œä½†ä¸å½±å“æ–‡çŒ®æœ¬èº«
            pmid = article.get('pmid', 'æœªçŸ¥')
            print(f"[WARN] æœŸåˆŠä¿¡æ¯å¢å¼ºå¤±è´¥ (PMID: {pmid}): {e}")
            
            # è®¾ç½®é»˜è®¤å€¼
            enhanced['cas_zone'] = None
            enhanced['impact_factor'] = None
            enhanced['jcr_quartile'] = None
            enhanced['_journal_info_error'] = str(e)
        
        return enhanced
    
    def print_filter_statistics(self, original_count: int, filtered_count: int, criteria: SearchCriteria):
        """æ‰“å°ç­›é€‰ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n=== ç­›é€‰ç»Ÿè®¡ ===")
        print(f"åŸå§‹æ–‡çŒ®æ•°: {original_count}")
        print(f"ç­›é€‰åæ–‡çŒ®æ•°: {filtered_count}")
        print(f"ç­›é€‰ç‡: {filtered_count/original_count*100:.1f}%" if original_count > 0 else "0%")
        
        # åˆ†åˆ«æ˜¾ç¤ºPubMedæ£€ç´¢æ¡ä»¶å’ŒæœŸåˆŠç­›é€‰æ¡ä»¶
        pubmed_filters = (criteria.year_start or criteria.year_end or criteria.keywords)
        journal_filters = (criteria.min_if or criteria.max_if or 
                          criteria.cas_zones or criteria.jcr_quartiles)
        
        if pubmed_filters:
            print("\n[PubMed] å·²åœ¨æ£€ç´¢ä¸­åº”ç”¨çš„æ¡ä»¶:")
            if criteria.year_start or criteria.year_end:
                print(f"  - å¹´ä»½: {criteria.year_start or 'ä¸é™'} - {criteria.year_end or 'ä¸é™'}")
            if criteria.keywords:
                print(f"  - å…³é”®è¯: {criteria.keywords}")
        
        if journal_filters:
            print("\n[FILTER] æœŸåˆŠç­›é€‰æ¡ä»¶:")
            if criteria.min_if or criteria.max_if:
                print(f"  - å½±å“å› å­: {criteria.min_if or 'ä¸é™'} - {criteria.max_if or 'ä¸é™'}")
            if criteria.cas_zones:
                print(f"  - ä¸­ç§‘é™¢åˆ†åŒº: {criteria.cas_zones}")
            if criteria.jcr_quartiles:
                print(f"  - JCRåˆ†åŒº: {criteria.jcr_quartiles}")
        
        if not pubmed_filters and not journal_filters:
            print("\nç­›é€‰æ¡ä»¶: æ— ")
            
        print("=" * 30)
    
    def export_filtered_results(self, articles: List[Dict], output_format: str = 'json', 
                              output_file: str = None) -> str:
        """
        å¯¼å‡ºç­›é€‰ç»“æœ
        
        Args:
            articles: ç­›é€‰åçš„æ–‡çŒ®åˆ—è¡¨
            output_format: è¾“å‡ºæ ¼å¼ ('json' æˆ– 'csv')
            output_file: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not articles:
            print("æ²¡æœ‰æ–‡çŒ®æ•°æ®å¯å¯¼å‡º")
            return ""
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if not output_file:
            from datetime import datetime
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"literature_search_{timestamp}"
        
        # æŒ‰æ‘˜è¦æ’åºï¼šæœ‰æ‘˜è¦çš„æ’åœ¨å‰é¢
        sorted_articles = sorted(articles, key=lambda x: (
            0 if x.get('abstract') and x.get('abstract').strip() else 1,  # æœ‰æ‘˜è¦çš„æ’å‰é¢
            x.get('pmid', '')  # åŒç­‰æƒ…å†µä¸‹æŒ‰PMIDæ’åº
        ))
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_data = []
        for i, article in enumerate(sorted_articles, 1):
            pmid = article.get('pmid', '')
            pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}" if pmid else ""
            
            export_item = {
                'åºå·': i,
                'æ–‡çŒ®æ ‡é¢˜': article.get('title', ''),
                'PMID': pmid,
                'æ–‡çŒ®åœ°å€': pubmed_url,
                'ä¸­ç§‘é™¢åˆ†åŒº': article.get('cas_zone', ''),
                'å½±å“å› å­': article.get('impact_factor', ''),
                'JCRåˆ†åŒº': article.get('jcr_quartile', ''),
                'æ‘˜è¦': article.get('abstract', ''),
                'ä½œè€…': article.get('authors_str', ''),
                'æœŸåˆŠ': article.get('journal', ''),
                'å‘è¡¨æ—¥æœŸ': article.get('publication_date', ''),
                'DOI': article.get('doi', '')
            }
            export_data.append(export_item)
        
        # è·å–ç»Ÿè®¡åˆ†ææ•°æ®
        analysis_stats = self.analyze_filtered_results(sorted_articles)
        
        # å¯¼å‡ºæ–‡ä»¶
        try:
            if output_format.lower() == 'json':
                output_path = f"{output_file}.json"
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        'export_info': {
                            'total_count': len(export_data),
                            'export_time': pd.Timestamp.now().isoformat(),
                            'has_abstract_count': sum(1 for a in sorted_articles if a.get('abstract') and a.get('abstract').strip())
                        },
                        'statistics': analysis_stats,  # æ·»åŠ ç»Ÿè®¡åˆ†ææ•°æ®
                        'articles': export_data
                    }, f, ensure_ascii=False, indent=2)
                    
            elif output_format.lower() == 'csv':
                output_path = f"{output_file}.csv"
                df = pd.DataFrame(export_data)
                df.to_csv(output_path, index=False, encoding='utf-8-sig')
                
                # ä¸ºCSVæ ¼å¼åˆ›å»ºå•ç‹¬çš„ç»Ÿè®¡æŠ¥å‘Šæ–‡ä»¶
                if analysis_stats:
                    stats_path = f"{output_file}_statistics.txt"
                    self._export_statistics_report(analysis_stats, stats_path)
                    print(f"ç»Ÿè®¡åˆ†æå·²å¯¼å‡ºåˆ°: {stats_path}")
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {output_format}")
            
            print(f"ç­›é€‰ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"å¯¼å‡ºå¤±è´¥: {e}")
            return ""
    
    def _export_statistics_report(self, analysis_stats: Dict, file_path: str):
        """å¯¼å‡ºç»Ÿè®¡åˆ†ææŠ¥å‘Šåˆ°æ–‡æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write("=" * 50 + "\n")
                f.write("æ–‡çŒ®ç­›é€‰ç»“æœç»Ÿè®¡åˆ†ææŠ¥å‘Š\n")
                f.write("=" * 50 + "\n\n")
                
                # æ€»ä½“ç»Ÿè®¡
                if "æ€»ä½“ç»Ÿè®¡" in analysis_stats:
                    stats = analysis_stats["æ€»ä½“ç»Ÿè®¡"]
                    f.write("[STAT] æ€»ä½“ç»Ÿè®¡:\n")
                    f.write(f"  æ€»æ–‡çŒ®æ•°: {stats.get('æ€»æ–‡çŒ®æ•°', 0)}\n")
                    f.write(f"  æœ‰æ‘˜è¦æ–‡çŒ®æ•°: {stats.get('æœ‰æ‘˜è¦æ–‡çŒ®æ•°', 0)}\n\n")
                    
                    if "è¦†ç›–ç‡ç»Ÿè®¡" in stats:
                        f.write("[TREND] è¦†ç›–ç‡ç»Ÿè®¡:\n")
                        for key, value in stats["è¦†ç›–ç‡ç»Ÿè®¡"].items():
                            f.write(f"  {key}: {value}\n")
                        f.write("\n")
                
                # å¹´ä»½åˆ†å¸ƒ
                if "å¹´ä»½åˆ†å¸ƒ" in analysis_stats and analysis_stats["å¹´ä»½åˆ†å¸ƒ"]:
                    f.write("ğŸ“… å¹´ä»½åˆ†å¸ƒ:\n")
                    for year, count in sorted(analysis_stats["å¹´ä»½åˆ†å¸ƒ"].items()):
                        f.write(f"  {year}: {count}ç¯‡\n")
                    f.write("\n")
                
                # ä¸­ç§‘é™¢åˆ†åŒºåˆ†å¸ƒ
                if "ä¸­ç§‘é™¢åˆ†åŒºåˆ†å¸ƒ" in analysis_stats and analysis_stats["ä¸­ç§‘é™¢åˆ†åŒºåˆ†å¸ƒ"]:
                    f.write("ğŸ† ä¸­ç§‘é™¢åˆ†åŒºåˆ†å¸ƒ:\n")
                    for zone, count in analysis_stats["ä¸­ç§‘é™¢åˆ†åŒºåˆ†å¸ƒ"].items():
                        f.write(f"  {zone}: {count}ç¯‡\n")
                    f.write("\n")
                
                # JCRåˆ†åŒºåˆ†å¸ƒ
                if "JCRåˆ†åŒºåˆ†å¸ƒ" in analysis_stats and analysis_stats["JCRåˆ†åŒºåˆ†å¸ƒ"]:
                    f.write("[LIST] JCRåˆ†åŒºåˆ†å¸ƒ:\n")
                    for quartile in ['Q1', 'Q2', 'Q3', 'Q4']:
                        if quartile in analysis_stats["JCRåˆ†åŒºåˆ†å¸ƒ"]:
                            count = analysis_stats["JCRåˆ†åŒºåˆ†å¸ƒ"][quartile]
                            f.write(f"  {quartile}: {count}ç¯‡\n")
                    f.write("\n")
                
                # å½±å“å› å­ç»Ÿè®¡
                if "å½±å“å› å­ç»Ÿè®¡" in analysis_stats and analysis_stats["å½±å“å› å­ç»Ÿè®¡"]:
                    f.write("[STAT] å½±å“å› å­ç»Ÿè®¡:\n")
                    for key, value in analysis_stats["å½±å“å› å­ç»Ÿè®¡"].items():
                        f.write(f"  {key}: {value}\n")
                    f.write("\n")
                
                f.write("=" * 50 + "\n")
                f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                
        except Exception as e:
            print(f"å¯¼å‡ºç»Ÿè®¡æŠ¥å‘Šå¤±è´¥: {e}")
    
    def analyze_filtered_results(self, articles: List[Dict]) -> Dict:
        """åˆ†æç­›é€‰ç»“æœï¼Œè¿”å›ç»Ÿè®¡æ•°æ®"""
        if not articles:
            print("æ²¡æœ‰æ–‡çŒ®æ•°æ®å¯åˆ†æ")
            return {}
        
        total = len(articles)
        print(f"\n=== ç­›é€‰ç»“æœåˆ†æ ===")
        
        # ç»Ÿè®¡æœŸåˆŠä¿¡æ¯è¦†ç›–ç‡
        has_cas = sum(1 for a in articles if a.get('cas_zone') is not None)
        has_if = sum(1 for a in articles if a.get('impact_factor') is not None)
        has_jcr = sum(1 for a in articles if a.get('jcr_quartile') is not None)
        has_abstract = sum(1 for a in articles if a.get('abstract') and a.get('abstract').strip())
        
        coverage_stats = {
            "ä¸­ç§‘é™¢åˆ†åŒºè¦†ç›–ç‡": f"{has_cas}/{total} ({has_cas/total*100:.1f}%)",
            "å½±å“å› å­è¦†ç›–ç‡": f"{has_if}/{total} ({has_if/total*100:.1f}%)", 
            "JCRåˆ†åŒºè¦†ç›–ç‡": f"{has_jcr}/{total} ({has_jcr/total*100:.1f}%)",
            "æ‘˜è¦è¦†ç›–ç‡": f"{has_abstract}/{total} ({has_abstract/total*100:.1f}%)"
        }
        
        print("æœŸåˆŠä¿¡æ¯è¦†ç›–ç‡:")
        print(f"  ä¸­ç§‘é™¢åˆ†åŒº: {coverage_stats['ä¸­ç§‘é™¢åˆ†åŒºè¦†ç›–ç‡']}")
        print(f"  å½±å“å› å­: {coverage_stats['å½±å“å› å­è¦†ç›–ç‡']}")
        print(f"  JCRåˆ†åŒº: {coverage_stats['JCRåˆ†åŒºè¦†ç›–ç‡']}")
        print(f"  æ‘˜è¦: {coverage_stats['æ‘˜è¦è¦†ç›–ç‡']}")
        
        # ç»Ÿè®¡å¹´ä»½åˆ†å¸ƒ
        years = []
        for article in articles:
            year = self._extract_year(article.get('publication_date', ''))
            if year:
                years.append(year)
        
        year_distribution = {}
        if years:
            print(f"\nå¹´ä»½åˆ†å¸ƒ:")
            year_counts = pd.Series(years).value_counts().sort_index()
            for year, count in year_counts.items():
                year_distribution[str(year)] = int(count)  # è½¬æ¢ä¸ºæ™®é€šintç±»å‹
                print(f"  {year}: {count}ç¯‡")
        
        # ç»Ÿè®¡ä¸­ç§‘é™¢åˆ†åŒºåˆ†å¸ƒ
        cas_zones = [a.get('cas_zone') for a in articles if a.get('cas_zone') is not None]
        cas_distribution = {}
        if cas_zones:
            print(f"\nä¸­ç§‘é™¢åˆ†åŒºåˆ†å¸ƒ:")
            zone_counts = pd.Series(cas_zones).value_counts().sort_index()
            for zone, count in zone_counts.items():
                cas_distribution[f"{zone}åŒº"] = int(count)  # è½¬æ¢ä¸ºæ™®é€šintç±»å‹
                print(f"  {zone}åŒº: {count}ç¯‡")
        
        # ç»Ÿè®¡JCRåˆ†åŒºåˆ†å¸ƒ
        jcr_quartiles = [a.get('jcr_quartile') for a in articles if a.get('jcr_quartile') is not None]
        jcr_distribution = {}
        if jcr_quartiles:
            print(f"\nJCRåˆ†åŒºåˆ†å¸ƒ:")
            quartile_counts = pd.Series(jcr_quartiles).value_counts()
            for quartile in ['Q1', 'Q2', 'Q3', 'Q4']:
                if quartile in quartile_counts:
                    jcr_distribution[quartile] = int(quartile_counts[quartile])  # è½¬æ¢ä¸ºæ™®é€šintç±»å‹
                    print(f"  {quartile}: {quartile_counts[quartile]}ç¯‡")
        
        # å½±å“å› å­ç»Ÿè®¡
        impact_factors = [a.get('impact_factor') for a in articles 
                         if a.get('impact_factor') is not None]
        impact_factor_stats = {}
        if impact_factors:
            print(f"\nå½±å“å› å­ç»Ÿè®¡:")
            impact_factor_stats = {
                "æœ€å°å€¼": round(min(impact_factors), 3),
                "æœ€å¤§å€¼": round(max(impact_factors), 3),
                "å¹³å‡å€¼": round(sum(impact_factors)/len(impact_factors), 3),
                "ä¸­ä½æ•°": round(sorted(impact_factors)[len(impact_factors)//2], 3)
            }
            for key, value in impact_factor_stats.items():
                print(f"  {key}: {value}")
        
        print("=" * 40)
        
        # è¿”å›å®Œæ•´çš„ç»Ÿè®¡åˆ†ææ•°æ®
        return {
            "æ€»ä½“ç»Ÿè®¡": {
                "æ€»æ–‡çŒ®æ•°": total,
                "æœ‰æ‘˜è¦æ–‡çŒ®æ•°": has_abstract,
                "è¦†ç›–ç‡ç»Ÿè®¡": coverage_stats
            },
            "å¹´ä»½åˆ†å¸ƒ": year_distribution,
            "ä¸­ç§‘é™¢åˆ†åŒºåˆ†å¸ƒ": cas_distribution,
            "JCRåˆ†åŒºåˆ†å¸ƒ": jcr_distribution,
            "å½±å“å› å­ç»Ÿè®¡": impact_factor_stats
        }


    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        cache_stats = self.journal_cache.get_stats() if hasattr(self, 'journal_cache') else {}
        
        return {
            'total_articles_processed': self.performance_stats['total_articles_processed'],
            'total_filter_time': self.performance_stats['total_filter_time'],
            'average_filter_time': self.performance_stats['total_filter_time'] / max(self.performance_stats['total_articles_processed'], 1),
            'cache_hits': self.performance_stats['cache_hits'],
            'parallel_batches': self.performance_stats['parallel_batches'],
            'memory_usage_mb': self.performance_stats['memory_usage_mb'],
            'errors': self.performance_stats['errors'],
            'cache_stats': cache_stats
        }
    
    def print_performance_report(self):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        report = self.get_performance_report()
        
        print("\n=== æ–‡çŒ®è¿‡æ»¤å™¨æ€§èƒ½æŠ¥å‘Š ===")
        print(f"å¤„ç†æ–‡çŒ®æ€»æ•°: {report['total_articles_processed']}")
        print(f"æ€»ç­›é€‰æ—¶é—´: {report['total_filter_time']:.2f}ç§’")
        print(f"å¹³å‡ç­›é€‰æ—¶é—´: {report['average_filter_time']:.4f}ç§’/ç¯‡")
        print(f"ç¼“å­˜å‘½ä¸­æ¬¡æ•°: {report['cache_hits']}")
        print(f"å¹¶è¡Œå¤„ç†æ‰¹æ¬¡æ•°: {report['parallel_batches']}")
        print(f"å†…å­˜ä½¿ç”¨: {report['memory_usage_mb']:.2f}MB")
        print(f"é”™è¯¯æ¬¡æ•°: {report['errors']}")
        
        if 'cache_stats' in report:
            cache_stats = report['cache_stats']
            print(f"ç¼“å­˜å¤§å°: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
            print(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.2%}")
        
        print("=" * 30)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†ç¼“å­˜
            if hasattr(self, 'journal_cache'):
                self.journal_cache.cache.clear()
                self.journal_cache.access_times.clear()
            
            # æ‰“å°æ€§èƒ½æŠ¥å‘Š
            if self.performance_stats['total_articles_processed'] > 0:
                self.print_performance_report()
                
        except Exception as e:
            print(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")


def test_literature_filter():
    """æµ‹è¯•æ–‡çŒ®ç­›é€‰å™¨"""
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_articles = [
        {
            'pmid': '12345',
            'title': 'Diabetes treatment with new drug',
            'issn': '0007-9235',  # CA-A CANCER JOURNALå½±å“å› å­232.4, Q1
            'eissn': '1542-4863',
            'publication_date': '2023-01-15',
            'abstract': 'This study investigates diabetes treatment...',
            'keywords_str': 'diabetes, treatment, drug'
        },
        {
            'pmid': '67890', 
            'title': 'COVID-19 vaccine effectiveness',
            'issn': '1234-5678',  # å‡è®¾çš„ä½å½±å“å› å­æœŸåˆŠ
            'eissn': '',
            'publication_date': '2021-05-20',
            'abstract': 'COVID-19 vaccine study...',
            'keywords_str': 'COVID-19, vaccine, effectiveness'
        }
    ]
    
    # åˆ›å»ºç­›é€‰æ¡ä»¶
    from intent_analyzer import SearchCriteria
    criteria = SearchCriteria(
        query="diabetes treatment",
        year_start=2020,
        year_end=2024,
        min_if=5.0,
        jcr_quartiles=['Q1']
    )
    
    # æµ‹è¯•ç­›é€‰
    filter_obj = LiteratureFilter()
    filtered = filter_obj.filter_articles(test_articles, criteria)
    
    filter_obj.print_filter_statistics(len(test_articles), len(filtered), criteria)
    filter_obj.analyze_filtered_results(filtered)
    
    # æµ‹è¯•å¯¼å‡º
    if filtered:
        filter_obj.export_filtered_results(filtered, 'json', 'test_filtered')




if __name__ == "__main__":
    test_literature_filter()